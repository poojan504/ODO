{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "poem_generator_training_haiku.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ravibhushan0487/ODO/blob/master/models/poem_generator/poem_generator_training_haiku.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0H6mjibI-54",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! git clone https://github.com/ravibhushan0487/chatbot.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKB1Pk9KZhdn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NddBfqvWUIpW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "    import keras\n",
        "except:\n",
        "    !pip install keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51m9N-5JU2z_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pathlib import Path\n",
        "\n",
        "import tensorflow as tf\n",
        "tf_session = tf.Session()\n",
        "from keras import backend as K\n",
        "K.set_session(tf_session)\n",
        "\n",
        "from keras.callbacks import ModelCheckpoint,  CSVLogger\n",
        "from keras.layers import Add, Dense, Input, LSTM\n",
        "from keras.models import Model\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import np_utils\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.externals import joblib\n",
        "\n",
        "# Local library with model definitions for training and generating\n",
        "# from chatbot.models import create_training_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMymTLrZeO-p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(preds, temperature=1.0):\n",
        "    # helper function to sample an index from a probability array\n",
        "    # From https://github.com/llSourcell/keras_explained/blob/master/gentext.py\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jJke0QteRGK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TrainingLine:\n",
        "    def __init__(self, name, previous_line, lstm, n_tokens):\n",
        "        self.char_input = Input(shape=(None, n_tokens), name='char_input_%s' % name)\n",
        "\n",
        "        self.syllable_input = Input(shape=(1,), name='syllable_input_%s' % name)\n",
        "        self.syllable_dense = Dense(lstm.units, activation='relu', name='syllable_dense_%s' % name)\n",
        "        self.syllable_dense_output = self.syllable_dense(self.syllable_input)\n",
        "\n",
        "        #self.lstm = LSTM(latent_dim, return_state=True, return_sequences=True, name='lstm_%s' % name)\n",
        "\n",
        "        if previous_line:\n",
        "            initial_state = [\n",
        "                Add(name='add_h_%s' % name)([\n",
        "                    previous_line.lstm_h,\n",
        "                    self.syllable_dense_output\n",
        "                ]),\n",
        "                Add(name='add_c_%s' % name)([\n",
        "                    previous_line.lstm_c,\n",
        "                    self.syllable_dense_output\n",
        "                ])\n",
        "            ]\n",
        "        else:\n",
        "            initial_state = [self.syllable_dense_output, self.syllable_dense_output]\n",
        "\n",
        "        self.lstm_out, self.lstm_h, self.lstm_c = lstm(self.char_input, initial_state=initial_state)\n",
        "\n",
        "        self.output_dense = Dense(n_tokens, activation='softmax', name='output_%s' % name)\n",
        "        self.output = self.output_dense(self.lstm_out)\n",
        "\n",
        "def create_training_model(latent_dim, n_tokens):\n",
        "    lstm = LSTM(latent_dim, return_state=True, return_sequences=True, name='lstm')\n",
        "    lines = []\n",
        "    inputs = []\n",
        "    outputs = []\n",
        "\n",
        "    for i in range(3):\n",
        "        previous_line = lines[-1] if lines else None\n",
        "        lines.append(TrainingLine('line_%s' % i, previous_line, lstm, n_tokens))\n",
        "        inputs += [lines[-1].char_input, lines[-1].syllable_input]\n",
        "        outputs.append(lines[-1].output)\n",
        "\n",
        "    training_model = Model(inputs, outputs)\n",
        "    training_model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "\n",
        "    return training_model, lstm, lines, inputs, outputs\n",
        "\n",
        "class GeneratorLine:\n",
        "    def __init__(self, name, training_line, lstm, n_tokens):\n",
        "        self.char_input = Input(shape=(None, n_tokens), name='char_input_%s' % name)\n",
        "\n",
        "        self.syllable_input = Input(shape=(1,), name='syllable_input_%s' % name)\n",
        "        self.syllable_dense = Dense(lstm.units, activation='relu', name='syllable_dense_%s' % name)\n",
        "        self.syllable_dense_output = self.syllable_dense(self.syllable_input)\n",
        "\n",
        "        self.h_input = Input(shape=(lstm.units,), name='h_input_%s' % name)\n",
        "        self.c_input = Input(shape=(lstm.units,), name='c_input_%s' % name)\n",
        "        initial_state = [self.h_input, self.c_input]\n",
        "\n",
        "        self.lstm = lstm\n",
        "\n",
        "        self.lstm_out, self.lstm_h, self.lstm_c = self.lstm(self.char_input, initial_state=initial_state)\n",
        "\n",
        "        self.output_dense = Dense(n_tokens, activation='softmax', name='output_%s' % name)\n",
        "        self.output = self.output_dense(self.lstm_out)\n",
        "\n",
        "        self.syllable_dense.set_weights(training_line.syllable_dense.get_weights())\n",
        "        #self.lstm.set_weights(lstm.get_weights())\n",
        "        self.output_dense.set_weights(training_line.output_dense.get_weights())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKeGzsvBbdsr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator:\n",
        "    def __init__(self, lstm, lines, tf_session, tokenizer, n_tokens, max_line_length):\n",
        "        self.tf_session = tf_session\n",
        "        self.tokenizer = tokenizer\n",
        "        self.n_tokens = n_tokens\n",
        "        self.max_line_length = max_line_length\n",
        "\n",
        "        self.lstm = LSTM(\n",
        "            lstm.units, return_state=True, return_sequences=True,\n",
        "            name='generator_lstm'\n",
        "        )\n",
        "        self.lines = [\n",
        "            GeneratorLine(\n",
        "                'generator_line_%s' % i,\n",
        "                lines[i], self.lstm, self.n_tokens\n",
        "            ) for i in range(3)\n",
        "        ]\n",
        "        self.lstm.set_weights(lstm.get_weights())\n",
        "\n",
        "    def generate_haiku(self, syllables=[5, 7, 5], temperature=.1, first_char=None):\n",
        "        output = []\n",
        "        h = None\n",
        "        c = None\n",
        "\n",
        "        if first_char is None:\n",
        "            first_char = chr(int(np.random.randint(ord('a'), ord('z')+1)))\n",
        "\n",
        "        next_char = self.tokenizer.texts_to_sequences(first_char)[0][0]\n",
        "\n",
        "        for i in range(3):\n",
        "            line = self.lines[i]\n",
        "            s = self.tf_session.run(\n",
        "                line.syllable_dense_output,\n",
        "                feed_dict={\n",
        "                    line.syllable_input: [[syllables[i]]]\n",
        "                }\n",
        "            )\n",
        "\n",
        "            if h is None:\n",
        "                h = s\n",
        "                c = s\n",
        "            else:\n",
        "                h = h + s\n",
        "                c = c + s\n",
        "\n",
        "            line_output = [next_char]\n",
        "\n",
        "            end = False\n",
        "            next_char = None\n",
        "            for i in range(self.max_line_length):\n",
        "                char, h, c = self.tf_session.run(\n",
        "                    [line.output, line.lstm_h, line.lstm_c],\n",
        "                    feed_dict={\n",
        "                        line.char_input: [[\n",
        "                            np_utils.to_categorical(\n",
        "                                line_output[-1],\n",
        "                                num_classes=self.n_tokens\n",
        "                            )\n",
        "                        ]],\n",
        "                        line.h_input: h,\n",
        "                        line.c_input: c\n",
        "                    }\n",
        "                )\n",
        "\n",
        "                char = sample(char[0,0], temperature)\n",
        "                if char == 1 and not end:\n",
        "                    end = True\n",
        "                if char != 1 and end:\n",
        "                    next_char = char\n",
        "                    char = 1\n",
        "\n",
        "                line_output.append(char)\n",
        "\n",
        "            cleaned_text = self.tokenizer.sequences_to_texts([\n",
        "                line_output\n",
        "            ])[0].strip()[1:].replace(\n",
        "                '   ', '\\n'\n",
        "            ).replace(' ', '').replace('\\n', ' ')\n",
        "\n",
        "            print(cleaned_text)\n",
        "            output.append(cleaned_text)\n",
        "\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGWhnDHBVAZ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Settings\n",
        "\n",
        "# Percent of samples to use for training, might be necessary if you're running out of memory\n",
        "sample_size = 1\n",
        "\n",
        "# The latent dimension of the LSTM\n",
        "latent_dim = 2048\n",
        "\n",
        "# Number of epochs to train for\n",
        "epochs = 20\n",
        "\n",
        "root_path = Path('/content')\n",
        "chatbot_path = root_path / 'chatbot'\n",
        "data_path = chatbot_path / 'data'\n",
        "input_path = data_path / 'input'\n",
        "poem_path = input_path / 'poems'\n",
        "haiku_path = poem_path / 'haikus.csv'\n",
        "\n",
        "name = 'all_data'\n",
        "output_dir = Path('output_%s' % name)\n",
        "output_dir.mkdir()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSVOLk1eVDxc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(str(haiku_path))\n",
        "df = df.sample(frac=sample_size)\n",
        "df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8H3omBRlVIUe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Duplicate lines with ambiguous syllable counts\n",
        "# (syllable counts where there is a comma because\n",
        "# multiple pronounciations are acceptable)\n",
        "\n",
        "lines = set([0, 1, 2])\n",
        "\n",
        "for i in range(3):\n",
        "    lines.remove(i)\n",
        "    df = df[[\n",
        "        '0', '1', '2',\n",
        "        #'1_syllables', '2_syllables'\n",
        "    ] + ['%s_syllables' % j for j in lines]].join(\n",
        "        df['%s_syllables' % i].str.split(\n",
        "            ',', expand=True\n",
        "        ).stack(-1).reset_index(\n",
        "            level=1, drop=True\n",
        "        ).rename('%s_syllables' % i)\n",
        "    ).drop_duplicates()\n",
        "    lines.add(i)\n",
        "\n",
        "df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZQUsYd2VLIr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Drop samples that are longer that the 99th percentile of length\n",
        "\n",
        "max_line_length = int(max([df['%s' % i].str.len().quantile(.99) for i in range(3)]))\n",
        "df = df[\n",
        "    (df['0'].str.len() <= max_line_length) & \n",
        "    (df['1'].str.len() <= max_line_length) & \n",
        "    (df['2'].str.len() <= max_line_length)\n",
        "].copy()\n",
        "df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icN1PmLsVOl3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pad the lines to the max line length with new lines\n",
        "for i in range(3):\n",
        "    # For input, duplicate the first character\n",
        "    # TODO - Why?\n",
        "    df['%s_in' % i] = (df[str(i)].str[0] + df[str(i)]).str.pad(max_line_length+2, 'right', '\\n')\n",
        "    \n",
        "    # \n",
        "    #df['%s_out' % i] = df[str(i)].str.pad(max_line_len, 'right', '\\n') + ('\\n' if i == 2 else df[str(i+1)].str[0])\n",
        "    \n",
        "    # TODO - trying to add the next line's first character before the line breaks\n",
        "    if i == 2: # If it's the last line\n",
        "        df['%s_out' % i] = df[str(i)].str.pad(max_line_length+2, 'right', '\\n')\n",
        "    else: \n",
        "        # If it's the first or second line, add the first character of the next line to the end of this line.\n",
        "        # This helps with training so that the next RNN has a better chance of getting the first character right.\n",
        "        df['%s_out' % i] = (df[str(i)] + '\\n' + df[str(i+1)].str[0]).str.pad(max_line_length+2, 'right', '\\n')\n",
        "    \n",
        "max_line_length += 2\n",
        "\n",
        "df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnkW3UxHVR61",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = df[['0_in', '1_in', '2_in']].values\n",
        "\n",
        "tokenizer = Tokenizer(filters='', char_level=True)\n",
        "tokenizer.fit_on_texts(inputs.flatten())\n",
        "n_tokens = len(tokenizer.word_counts) + 1\n",
        "\n",
        "# X is the input for each line in sequences of one-hot-encoded values\n",
        "X = np_utils.to_categorical([\n",
        "    tokenizer.texts_to_sequences(inputs[:,i]) for i in range(3)\n",
        "], num_classes=n_tokens)\n",
        "outputs = df[['0_out', '1_out', '2_out']].values\n",
        "\n",
        "# Y is the output for each line in sequences of one-hot-encoded values\n",
        "Y = np_utils.to_categorical([\n",
        "    tokenizer.texts_to_sequences(outputs[:,i]) for i in range(3)\n",
        "], num_classes=n_tokens)\n",
        "\n",
        "# X_syllables is the count of syllables for each line\n",
        "X_syllables = df[['0_syllables', '1_syllables', '2_syllables']].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KP1m4mg9VVVd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "joblib.dump([latent_dim, n_tokens, max_line_length, tokenizer], str(output_dir / 'metadata.pkl'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qssVrwnfVbfc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_model, lstm, lines, inputs, outputs = create_training_model(latent_dim, n_tokens)\n",
        "\n",
        "filepath = str(output_dir / (\"%s-{epoch:02d}-{loss:.2f}-{val_loss:.2f}.hdf5\" % latent_dim))\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "\n",
        "csv_logger = CSVLogger(str(output_dir / 'training_log.csv'), append=True, separator=',')\n",
        "\n",
        "callbacks_list = [checkpoint, csv_logger]\n",
        "training_model.fit([\n",
        "    X[0], X_syllables[:,0], \n",
        "    X[1], X_syllables[:,1], \n",
        "    X[2], X_syllables[:,2]\n",
        "], [Y[0], Y[1], Y[2]], batch_size=64, epochs=epochs, validation_split=.1, callbacks=callbacks_list)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ns45-CEVga-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator = Generator(lstm, lines, tf_session, tokenizer, n_tokens, max_line_length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jqLOpqJVh_z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator.generate_haiku()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}